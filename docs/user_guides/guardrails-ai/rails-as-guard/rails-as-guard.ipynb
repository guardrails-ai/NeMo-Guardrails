{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guardrails as Guards\n",
    "This guide will teach you how to add NeMo Guardrails to a GuardrailsAI Guard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init: remove any existing configuration\n",
    "! rm -r config\n",
    "! mkdir config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "We'll be using an OpenAI model for our LLM in this guide, so set up an OpenAI API key, if not already set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export OPENAI_API_KEY=$OPENAI_API_KEY    # Replace with your own key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're running this inside a notebook, you also need to patch the AsyncIO loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Guardrails\n",
    "We'll start by creating a new guardrails configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config/config.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/config.yml\n",
    "models:\n",
    " - type: main\n",
    "   engine: openai\n",
    "   model: gpt-3.5-turbo-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a quick test to make sure everything is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0d1ffa109e4961b4d6e19007d676a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import RailsConfig, LLMRails\n",
    "\n",
    "config = RailsConfig.from_path(\"./config\")\n",
    "rails = LLMRails(config)\n",
    "\n",
    "response = rails.generate(\"Hello!\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked!  Now let's install a validator from the GuardrailsAI Hub to augment our guardrails configuration from above.\n",
    "\n",
    "If you haven't already, install and configure guardrails-ai before trying to install the DetectPII validator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install guardrails-ai\n",
    "! guardrails configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! guardrails hub install hub://guardrails/detect_pii --no-install-local-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the rails defined earlier as the basis for our Guard.  We'll also attach the DetectPII validator as an additional measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrails.integrations.nemoguardrails import NemoguardrailsGuard\n",
    "from guardrails.hub import DetectPII\n",
    "\n",
    "\n",
    "guard = NemoguardrailsGuard(rails)\n",
    "guard.use(DetectPII(\n",
    "    pii_entities=[\"PERSON\", \"EMAIL_ADDRESS\"],\n",
    "    on_fail=\"fix\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "With everything configured, we can test out our new Guard!\n",
    "\n",
    "Let's invoke the Guard with a message that prompts the LLM to return personal information like names, email addresses, etc.. Since we specified `on_fail=\"fix\"` in the DetectPII validator, the response should have any PII filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current president of the United States is <PERSON>. His email address is <EMAIL_ADDRESS>. He can also be reached through his personal email at <EMAIL_ADDRESS>. Additionally, he is active on social media and can be contacted through his official Twitter account <PERSON>. Is there anything else you would like to know about President <PERSON>?\n"
     ]
    }
   ],
   "source": [
    "response = guard(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Who is the current president of the United States, and what was their email address?\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response.validated_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can see that the Guard called the LLM configured in the LLMRails, validated the output, and filtered it accordingly. If however, we prompt the LLM with a message that does not cause it to return PII, we should get the unaltered response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there! It's nice to meet you. My name is AI Assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "response = guard(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Hello!\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response.validated_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
